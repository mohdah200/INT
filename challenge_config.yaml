title: "Classification Accuracy Challenge"
short_description: "Classification task to evaluate accuracy."
description: "templates/description.html"
evaluation_details: "templates/evaluation_details.html"
terms_and_conditions: "templates/terms_and_conditions.html"
image: "logo.jpg"
submission_guidelines: "templates/submission_guidelines.html"
leaderboard_description: "Leaderboard showing accuracy of submissions."
evaluation_script: "evaluation_script.zip"
remote_evaluation: False
is_docker_based: False
start_date: "2024-07-20 08:00:00"
end_date: "2025-01-21 07:59:59"
published: True

leaderboard:
  - id: 1
    schema:
      labels: ["Accuracy"]
      default_order_by: "Accuracy"
      metadata:
        "Accuracy": {
          "sort_ascending": False,
          "description": "Classification Accuracy"
        }

challenge_phases:
  - id: 1
    name: "Single Phase"
    description: "templates/challenge_phase_description.html"
    leaderboard_public: True
    is_public: True
    is_submission_public: True
    start_date: "2024-07-20 08:00:00"
    end_date: "2025-01-21 07:59:59"
    test_annotation_file: "annotations/ground_truth.csv"
    codename: "single_phase"
    max_submissions_per_day: 100
    max_submissions_per_month: 1000
    max_submissions: 10000
    default_submission_meta_attributes:
      - name: method_name
        is_visible: True
      - name: method_description
        is_visible: True
      - name: project_url
        is_visible: True
      - name: publication_url
        is_visible: True
    allowed_submission_file_types: ".csv"

dataset_splits:
  - id: 1
    name: "Test Split"
    codename: "test_split"

challenge_phase_splits:
  - challenge_phase_id: 1
    leaderboard_id: 1
    dataset_split_id: 1
    visibility: 3
    leaderboard_decimal_precision: 2
    is_leaderboard_order_descending: False
